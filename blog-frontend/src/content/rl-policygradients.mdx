---
title: "Policy Gradients in RL"
description: "Explore policy gradient methods, their advantages, and visualize policy improvement."
date: "2024-05-21"
---

# Policy Gradients in RL

Policy Gradient methods are a family of reinforcement learning algorithms that optimize the policy directly. Instead of learning value functions, they adjust the parameters of the policy to maximize expected reward.

## Why Policy Gradients?

- Can handle continuous action spaces
- Directly optimize stochastic policies

## Common Algorithms

- REINFORCE
- Actor-Critic

## Policy Gradient Update Rule

```math
\nabla_\theta J(\theta) = \mathbb{E}_\pi [\nabla_\theta \log \pi_\theta(a|s) R]
```

## Example: Policy Update (Python)

```python
policy_loss = -torch.mean(log_probs * returns)
optimizer.zero_grad()
policy_loss.backward()
optimizer.step()
```

## Chart: Policy Improvement

<PolicyChart />

Policy gradients are widely used in deep RL and robotics.
